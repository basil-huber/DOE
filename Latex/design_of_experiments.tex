\section{Design of Experiments}

We design the experiment to efficiently determine the influence of the  parameters $a$, $b$, and $d$ on the mean travel time and the mean total jerk experienced during the journey.

\subsection{Choosing a Model}
The complex nature of the problem does not allow to make assumptions on the model underlying the experiments. We therefore choose to investigate several models. We begin with a simple linear model of the form
\begin{subequations}\label{eq:model_lin}
\begin{align}
	y_a &= a_0 + \displaystyle\sum_{i=1}^{3} a_i x_i + \varepsilon_a \quad \text{and}\\ 
	y_b &= b_0 + \displaystyle\sum_{i=1}^{3} b_i x_i + \varepsilon_b,
\end{align}
\end{subequations}
where $y_a$ and $y_b$ are the responses (mean travel time and mean total jerk), $a_i$ and $b_i$ are the effects of the $i$th parameter on the travel time and the jerk respectively, $x_1$, $x_2$ and $x_3$ are the values taken by the parameters $a$, $b$ and $d$ respectively. Since the results of the analysis suggest that the influence of parameter $a$ on the travel time is negligible (cf. section~\ref{sec:results}), we adjust our model by omitting this parameter.
The nature of the algorithm suggest that interactions could play an important role. Hence, we will also investigate a linear model allowing for interactions between the parameters as described in eq.~\ref{eq:model_inter}.
\begin{subequations}\label{eq:model_inter}
\begin{align}
	y_a &= a_0 + \displaystyle\sum_{i=1}^{3} a_i x_i + \displaystyle\sum_{i,j=1, i \neq j}^{3} a_{ij} x_i x_j + \varepsilon_a \quad \text{and}\\ 
	y_b &= b_0 + \displaystyle\sum_{i=1}^{3} b_i x_i + \displaystyle\sum_{i,j=1, i \neq j}^{3} b_{ij} x_i x_j + \varepsilon_b
\end{align}
\end{subequations}
As for the linear model, we found that the influence of parameter $a$ and its interactions is negligible and omit the parameter for the travel time. We also test a quadratic model with the following equations:
\begin{subequations}\label{eq:model_quadr}
\begin{align}
	y_a &= a_0 + \displaystyle\sum_{i=1}^{3} a_i x_i + \displaystyle\sum_{i,j=1}^{3} a_{ij} x_i x_j + \varepsilon_a \quad \text{and}\\ 
	y_b &= b_0 + \displaystyle\sum_{i=1}^{3} b_i x_i + \displaystyle\sum_{i,j=1}^{3} b_{ij} x_i x_j + \varepsilon_b,
\end{align}
\end{subequations}
which has additionally the quadratic factors $a_{ii}$.
Due to the limited number of experiments, we choose not to test higher order models.

\subsection{Choosing the Matrix of Experiments}
The matrix of experiments $E$ contains the levels, i.e., the values taken by the factors, normalized between $-1$ and $1$. Each row of $E$ corresponds to an experiment and each column to a factor. The choice of the matrix of experiments is crucial for an efficient estimation of the effects. While a high number of experiments $N$ yields a more accurate fit, experiments with flying platforms are costly in terms of time. The limits of the levels shown in tbl.~\ref{tbl:levels} were determined in preliminary experiment.
\begin{table}[h!]
	\centering
	\begin{tabular}{l r r r}
	 & $x_1$ (a) & $x_2$ (b) & $x_3$ (d) \\\hline
	min & 1 & 5 & 800\\
	max & 13 & 13 & 1400 \\
	\end{tabular}
	\caption{Maximal and minimal levels for each factor, found in perliminary experiments}\label{tbl:levels}
\end{table}
\begin{figure}[h]
    \centering
	\setlength{\abovecaptionskip}{1pt plus 3pt minus 0pt}
	\setlength{\figH}{0.25\textwidth}
	\input{images/box_behnken.tex}
    \caption{Chosen levels for the experiment: Original Box-Behnken design (in blue) and adjusted design (in red);}\label{fig:design}
\end{figure} 

The additional constraint $ x_1 \le x_2 $ is introduced by eq.~\ref{eq:phi}. To well cover the central values, we choose a Box-Behnken design~\cite{box_behnken} as shown in fig.~\ref{fig:design}. We adjust the values to fit our constraint by moving the points to the center of the sides of the truncated cube. The central point was moved to the center of gravity of the truncated cube. This results in the following matrix of experiments:
\begin{equation}
E = \begin{pmatrix}
		x_{11}  &  x_{12}  & x_{13} \\
		x_{21}  &  x_{22}  & x_{23} \\
		\vdots & \vdots & \vdots \\
		x_{N1} & x_{N2} & x_{N3}
		\end{pmatrix} = \begin{pmatrix}
		-1  &  0  & -1 \\
		-1  &  0  & -1 \\
		\nicefrac{1}{3} &	0	& -1 \\
		\nicefrac{1}{3} &	0	& 1  \\
		-1	&	-1	& 0 \\
		-1	&	1	& 0 \\
		-\nicefrac{1}{3} &	-1	& 0 \\
		1	  & -1  & 0 \\
		-\nicefrac{2}{3} &	-1	& -1 \\
		0	  &  1	& -1 \\
		-\nicefrac{2}{3} &	-1	& 1 \\
		-0.28 & 0.17 & 0 \\
		0 	& 1 	& 1 
		\end{pmatrix}
\end{equation}



\subsection{Estimating the effects}
We call $X$ the matrix of the model. Each row contains the levels of the factors and their products (if any) for one run of the experiment. For the quadratic model, $X$ takes the following form:
\begin{equation}
	\begin{pmatrix}
	1 & x_{11} & x_{12} & x_{13} & x_{11}^2 & x_{11} x_{12} & \cdots & x_{13}^2 \\
	1 & x_{21} & x_{22} & x_{23} & x_{21}^2 & x_{21} x_{22} & \cdots & x_{23}^2 \\
	 \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
	1 & x_{N1} & x_{N2} & x_{N3} & x_{N1}^2 & x_{N1} x_{N2} & \cdots & x_{N3}^2 \\
	\end{pmatrix}
\end{equation}

The column vectors $Y_a$ and $Y_b$ contain the response in travel time and jerk where each element corresponds to the response to one run of the experiment.
Eq.~\ref{eq:model_lin}~to~\ref{eq:model_quadr} can then be formulated for all experiments as
\begin{equation}
 Y = X \hat{\alpha} + R = \hat{Y} + R,
\end{equation}
where $\hat{\alpha}$ is the column vector containing the estimated effects, $\hat{Y}$ are the estimated responses and $R$ is the residue column vector.

We estimate the effects (coefficients of the model) by minimizing the the residue in the sense of least square as expressed by eq. \ref{eq:least_square}.
\begin{equation}\label{eq:least_square}
	\hat\alpha =  \argmin_{\alpha}(\|Y - \hat{Y}\|^2) = \argmin_{\alpha}(\|R\|^2).
\end{equation}
The solution to eq. \ref{eq:least_square} can be computed as
\begin{equation}
	\hat\alpha = (X^T X)^{-1} X^{T} Y.
\end{equation}



\subsection{Analysis of Variance}
To evaluate our model, we use an F-test. We therefore define the sum of squares ($\text{SS}$) as
\begin{equation}
	\text{SS}_\text{eff} = \| \hat{Y} \|^2 = \| X \hat{\alpha} \|^2
\end{equation}
for the effects and as
\begin{equation}
	\text{SS}_\text{res} = \| R \|^2 = \| Y - \hat{Y} \| = \| Y - X \hat{\alpha} \|^2.
\end{equation}
The mean of squares ($\text{MS}$) as the distribution of the sum of squares ($\text{SS}$) over the degree's of freedom $\text{df}$:
\begin{equation}
	\text{MS}_\text{eff} = \frac{\text{SS}_\text{eff}}{\text{df}_\text{eff}} \quad \text{and} \quad 
	\text{MS}_\text{res} = \frac{\text{SS}_\text{res}}{\text{df}_\text{res}},
\end{equation}
where the degrees of freedom of the effects $\text{df}_\text{eff}$ is the number of effects including the constant and the degrees of freedom of the residue $\text{df}_\text{res}$ equals the number of experiment $N$ minus the degrees of freedom of the effects:
\begin{equation}
\text{df}_\text{res} = N - \text{df}_\text{eff}.
\end{equation}
We define Fisher's coefficient $F$ as follows:
\begin{equation}
F = \frac{\text{MS}_\text{eff}}{\text{MS}_\text{res}}.
\end{equation}

Using Fisher's coefficient, we can express the probability $p$ of this ratio occurring at random as the cumulative distribution function at $F$ for the degrees of freedom $\text{df}_\text{eff}$ and $\text{df}_\text{res}$:
\begin{equation}
	p = \int_{-\inf}^F f(t,\text{df}_\text{eff},\text{df}_\text{res}) \mathrm{d}t,
\end{equation}
where $f(x,\text{df}_1,\text{df}_2)$ is the probability density function of an F-distributed variable. Hence, the lower the p-value, the better is the model. We use the p-value as a metric to measure the performance of our model and compare it to the confidence level $\alpha_\text{conf} = 0.05$.
